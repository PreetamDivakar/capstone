# Hybrid Cloud Orchestrator - Complete Project Explanation

## üìã Table of Contents
1. [Project Overview](#project-overview)
2. [Core Purpose](#core-purpose)
3. [Architecture & Components](#architecture--components)
4. [Functionalities](#functionalities)
5. [Installation & Setup](#installation--setup)
6. [Running the Project](#running-the-project)
7. [Testing the Project](#testing-the-project)
8. [Project Workflow](#project-workflow)
9. [Important Steps & Best Practices](#important-steps--best-practices)
10. [Configuration](#configuration)
11. [Notes](#notes)
12. [Learning Outcomes](#learning-outcomes)

---

## üéØ Project Overview

The **Hybrid Cloud Orchestrator** is an intelligent decision-making system that analyzes workload characteristics and recommends the optimal deployment platform (Traditional Infrastructure vs. Serverless) based on cost, performance, and resource requirements. It combines machine learning models with rule-based heuristics to provide cost-effective cloud deployment recommendations.

---

## üéØ Core Purpose

The project is designed to:

1. **Automate Platform Selection**: Automatically determine whether a workload should run on traditional infrastructure (VMs/containers) or serverless platforms (AWS Lambda, Azure Functions, etc.)

2. **Cost Optimization**: Calculate and compare costs between traditional and serverless deployments to minimize cloud spending

3. **Workload Analysis**: Process workload characteristics including:
   - CPU cores required
   - Memory requirements (MB)
   - Execution time (seconds)
   - Data size (MB)
   - Latency sensitivity

4. **Deployment Automation**: Generate ready-to-deploy configuration files (Docker, Serverless Framework, etc.) based on recommendations

5. **Performance Benchmarking**: Provide comparative metrics and visualizations for different deployment strategies

---

## üèóÔ∏è Architecture & Components

### Project Structure

```
hybrid-cloud-orchestrator/
‚îú‚îÄ‚îÄ backend/              # Flask web application
‚îÇ   ‚îú‚îÄ‚îÄ app.py           # Main Flask server with API endpoints
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt # Python dependencies
‚îú‚îÄ‚îÄ frontend/            # Web UI
‚îÇ   ‚îú‚îÄ‚îÄ templates/       # HTML templates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html   # Main workload input page
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ benchmark.html # Benchmark dashboard
‚îÇ   ‚îî‚îÄ‚îÄ static/          # CSS and JavaScript
‚îÇ       ‚îú‚îÄ‚îÄ app.js       # Main frontend logic
‚îÇ       ‚îú‚îÄ‚îÄ benchmark.js # Benchmark visualization
‚îÇ       ‚îî‚îÄ‚îÄ styles.css   # Styling
‚îú‚îÄ‚îÄ model/               # Machine Learning components
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py   # Model training script
‚îÇ   ‚îú‚îÄ‚îÄ run_model.py     # Model execution wrapper
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator_model.pkl # Trained model (generated)
‚îú‚îÄ‚îÄ preprocessing/       # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py    # Data preprocessing script
‚îÇ   ‚îî‚îÄ‚îÄ run.py          # Preprocessing wrapper
‚îú‚îÄ‚îÄ data/               # Datasets
‚îÇ   ‚îú‚îÄ‚îÄ Sample_Dataset.csv
‚îÇ   ‚îî‚îÄ‚îÄ workload_dataset_sample.csv (generated)
‚îî‚îÄ‚îÄ tests/              # Test suite
    ‚îî‚îÄ‚îÄ test_deploy.py  # Deployment bundle tests
```

### Technology Stack

- **Backend**: Python 3.11, Flask
- **Frontend**: HTML5, CSS3, JavaScript (Vanilla), Chart.js
- **Machine Learning**: scikit-learn, pandas, RandomForestClassifier
- **Data Processing**: pandas, numpy
- **Testing**: pytest

---

## ‚öôÔ∏è Functionalities

### 1. **Workload Prediction & Recommendation** (`/predict` endpoint)

**Purpose**: Analyzes workload parameters and recommends the optimal deployment platform.

**Input Parameters**:
- `cpu_cores`: Number of CPU cores required (float)
- `memory_mb`: Memory requirement in megabytes (float)
- `latency_sensitive`: Binary flag (0 or 1) indicating if workload is latency-sensitive
- `execution_time`: Expected execution time in seconds (float)
- `data_size_mb`: Data size in megabytes (float)

**Output**:
- Cost comparison (Traditional vs. Serverless)
- Cost ratio
- Recommended platform (Serverless or Traditional)
- All input parameters (normalized)

**Decision Logic**:
- Serverless is recommended if:
  - Execution time ‚â§ 300 seconds
  - CPU cores ‚â§ 2
  - Memory ‚â§ 2048 MB
  - Not latency-sensitive (latency_sensitive = 0)
  - Cost ratio < 5.0

- Traditional is recommended otherwise

**Cost Calculation**:
- **Traditional**: `((CPU √ó $0.0316/core-hour) + (Memory_GB √ó $0.0045/GB-hour)) √ó Runtime_hours`
- **Serverless**: `$0.0000002/request + ($0.00001667/GB-second √ó Memory_GB √ó Runtime_seconds)`

### 2. **Deployment Bundle Generation** (`/deploy` endpoint)

**Purpose**: Generates ready-to-deploy configuration files based on workload requirements.

**Output Files** (ZIP bundle):
- `README.md`: Deployment instructions
- `Dockerfile`: Container configuration
- `docker-compose.yml`: Docker Compose configuration with resource limits
- `deploy_local.sh`: Local deployment script

**For Serverless Platform**:
- `lambda_handler.py`: AWS Lambda handler function
- `serverless.yml`: Serverless Framework configuration

**For Traditional Platform**:
- `run_job.py`: Job execution script

**Features**:
- Automatically configures CPU and memory limits
- Sets appropriate timeouts
- Includes cost calculation logic
- Platform-specific optimizations

### 3. **Benchmark Dashboard** (`/benchmark` endpoint)

**Purpose**: Visualizes comparative metrics across different deployment platforms.

**Features**:
- **Interactive Charts**: Bar charts comparing Cost, Latency, and Efficiency
- **Confusion Matrix**: Classification performance visualization
- **Performance Metrics**: Precision, Recall, F1 Score, Accuracy
- **Summary Statistics**: Average cost, percentile latencies

**Metrics Displayed**:
- Cost comparison ($)
- Latency comparison (ms)
- Efficiency percentage (%)
- Classification accuracy metrics

### 4. **Data Preprocessing** (`preprocessing/preprocess.py`)

**Purpose**: Transforms raw workload data (e.g., Google Cluster Trace) into a clean, standardized format suitable for machine learning model training and analysis.

#### **Detailed Preprocessing Pipeline**

##### **Step 1: Data Loading & Sampling**
- **Input**: Raw CSV file (e.g., `Sample_Dataset.csv`)
- **Process**: 
  - Reads entire dataset as strings to safely handle complex JSON-like fields
  - Randomly samples 100 rows (or all rows if dataset has < 100 rows)
  - Uses optional random seed for reproducibility
- **Why**: Large datasets can be computationally expensive; sampling provides representative subset

##### **Step 2: Resource Request Parsing** (`parse_resource_request()`)
- **Purpose**: Extracts CPU and memory requirements from various input formats
- **Handles Multiple Formats**:
  - **Dictionary format**: `{"cpu": 0.5, "memory": 0.25}` (normalized 0-1)
  - **String format**: `"cpu: 0.5, memory: 0.25"`
  - **Numeric pairs**: `"0.5, 0.25"`
  - **Single values**: Interprets based on magnitude
- **Normalization Logic**:
  - If value ‚â§ 1.0: Treated as fraction, multiplied by machine capacity
  - If value > 1.0: Treated as absolute value
  - Memory < 256: Assumed to be in GB, converted to MB
  - Memory ‚â• 256: Assumed to be in MB
- **Machine Configuration**: Defaults to 16 cores, 65536 MB (64 GB), configurable via CLI

##### **Step 3: Memory Gap Filling**
- **Process**: If memory is missing from resource_request, attempts to fill from `assigned_memory` column
- **Logic**: 
  - Checks if `assigned_memory` exists in dataset
  - Applies same normalization rules (fraction vs absolute)
  - Handles various units (GB, MB) automatically

##### **Step 4: Execution Time Calculation** (`epoch_to_seconds()`)
- **Purpose**: Calculates workload execution time from start/end timestamps
- **Auto-Detection of Time Units**:
  - **Nanoseconds**: Values > 1e17 ‚Üí divide by 1e9
  - **Microseconds**: Values > 1e14 ‚Üí divide by 1e6
  - **Milliseconds**: Values > 1e11 ‚Üí divide by 1e3
  - **Seconds**: Values ‚â§ 1e11 ‚Üí use as-is
- **Validation**: Removes negative or zero execution times (invalid data)
- **Why**: Different datasets use different timestamp formats; auto-detection ensures compatibility

##### **Step 5: Latency Sensitivity Derivation** (`derive_latency_sensitive()`)
- **Purpose**: Determines if workload is latency-sensitive (binary flag: 0 or 1)
- **Heuristic Rules**:
  1. **Priority-based**: If priority ‚â§ 2 ‚Üí latency-sensitive
  2. **Scheduling Class**: If scheduling_class ‚â§ 1 ‚Üí latency-sensitive
  3. **Name-based**: If collection_type or collection_name contains keywords:
     - "api", "realtime", "interactive", "latency", "foreground"
     - ‚Üí latency-sensitive
- **Default**: Returns 0 (not latency-sensitive) if no indicators found
- **Why**: Latency-sensitive workloads typically require traditional infrastructure for better control

##### **Step 6: Data Size Estimation**
- **Process**: 
  - First checks for existing `data_size_mb` or `data_size` columns
  - If missing, estimates using formula: `memory_mb √ó (execution_time / 100)`
  - Fills missing values with 0
- **Why**: Data size affects transfer costs and deployment strategy

##### **Step 7: Cost Calculation**
- **Traditional Infrastructure Cost**:
  ```
  cost_traditional = ((CPU_cores √ó $0.0316/core-hour) + (Memory_GB √ó $0.0045/GB-hour)) √ó Runtime_hours
  ```
- **Serverless Cost**:
  ```
  cost_serverless = $0.0000002/request + ($0.00001667/GB-second √ó Memory_GB √ó Runtime_seconds)
  ```
- **Cost Ratio**: `cost_serverless / cost_traditional` (used for decision-making)
- **Handling Edge Cases**: Adds small epsilon (1e-9) to prevent division by zero

##### **Step 8: Failure Flag Normalization** (`normalize_failed()`)
- **Purpose**: Standardizes failure indicators to boolean
- **Recognized Values**: "1", "true", "t", "yes", "y", "fail", "failed", "failure"
- **Why**: Failed workloads may indicate need for traditional infrastructure with more control

##### **Step 9: Target Platform Classification** (`heuristic_target()`)
- **Purpose**: Creates ground truth labels for model training
- **Serverless Classification Rules** (ALL must be true):
  - Execution time ‚â§ 300 seconds (5 minutes)
  - CPU cores ‚â§ 2
  - Memory ‚â§ 2048 MB (2 GB)
  - Cost ratio < 5.0
  - NOT latency-sensitive (latency_sensitive = 0)
  - NOT previously failed
- **Traditional Classification**: Default if any serverless condition fails
- **Why**: These rules reflect real-world serverless platform constraints (timeouts, memory limits)

##### **Step 10: Data Cleaning & Output**
- **Removes**:
  - Rows with infinite values
  - Rows with NaN in critical fields
- **Standardizes Column Names**: Lowercase with spaces
- **Output Format**: Clean CSV with standardized columns

#### **Output Columns** (Final Dataset):
- **Features**: `cpu cores`, `memory mb`, `latency sensitive`, `execution time`, `data size mb`
- **Cost Metrics**: `cost traditional`, `cost serverless`, `cost ratio`
- **Target**: `target platform` (serverless/traditional)

#### **Preprocessing Command Examples**:

```bash
# Basic preprocessing
cd preprocessing
python preprocess.py --input ../data/Sample_Dataset.csv --output ../data/workload_dataset_sample.csv

# Custom machine configuration
python preprocess.py --input ../data/Sample_Dataset.csv --output ../data/workload_dataset_sample.csv --cores 32 --mem_mb 131072

# With random seed for reproducibility
python preprocess.py --input ../data/Sample_Dataset.csv --output ../data/workload_dataset_sample.csv --seed 42
```

#### **Preprocessing Output**:
- Console output shows:
  - Number of rows sampled
  - Machine configuration used
  - Timestamp unit detection messages
  - Preview of first 10 processed rows
- Generated file: `workload_dataset_sample.csv` ready for model training

### 5. **Machine Learning Model Training** (`model/train_model.py`)

**Purpose**: Trains a RandomForest classifier to predict optimal deployment platform (serverless vs traditional) based on workload characteristics.

#### **Training Pipeline Overview**

##### **Step 1: Data Loading & Preparation**
- **Input**: Preprocessed CSV file from `preprocess.py`
- **Data Cleaning**:
  - Removes rows with missing `target platform` values
  - Replaces infinite values (inf, -inf) with NaN and drops them
  - Ensures all numeric features are valid
- **Why**: ML models require clean, complete data

##### **Step 2: Feature Selection**
- **Selected Features** (6 features):
  1. `cpu cores` - Number of CPU cores required
  2. `memory mb` - Memory requirement in megabytes
  3. `latency sensitive` - Binary flag (0 or 1)
  4. `execution time` - Runtime in seconds
  5. `data size mb` - Data size in megabytes
  6. `cost ratio` - Ratio of serverless to traditional cost
- **Why These Features**: 
  - Directly related to platform constraints (CPU, memory, time)
  - Cost ratio captures economic efficiency
  - Latency sensitivity indicates infrastructure requirements

##### **Step 3: Target Encoding**
- **Process**: Maps string labels to numeric values
  - `"serverless"` ‚Üí 0
  - `"traditional"` ‚Üí 1
- **Why**: Scikit-learn requires numeric target labels

##### **Step 4: Train-Test Split**
- **Method**: `train_test_split()` with stratification
- **Parameters**:
  - `test_size`: 0.2 (20% for testing, 80% for training)
  - `random_state`: 42 (ensures reproducibility)
  - `stratify=y`: Maintains class distribution in both sets
- **Why Stratification**: Prevents imbalanced splits that could bias evaluation

##### **Step 5: Model Initialization**
- **Algorithm**: `RandomForestClassifier` from scikit-learn
- **Hyperparameters**:
  - `n_estimators=200`: Number of decision trees in the forest
    - More trees = better performance but slower training
    - 200 provides good balance
  - `class_weight="balanced"`: Automatically adjusts weights inversely proportional to class frequencies
    - Critical for imbalanced datasets
    - Prevents model from always predicting majority class
  - `max_depth=None`: No limit on tree depth
    - Allows trees to fully grow until all leaves are pure
    - Prevents underfitting
  - `random_state=42`: Ensures reproducible results
  - `n_jobs=-1`: Uses all available CPU cores for parallel training
- **Why RandomForest**: 
  - Handles non-linear relationships
  - Provides feature importance scores
  - Robust to outliers
  - Works well with mixed data types

##### **Step 6: Model Training**
- **Process**: `model.fit(X_train, y_train)`
- **What Happens**:
  - Builds 200 decision trees
  - Each tree sees a random subset of data (bootstrap sampling)
  - Each split considers a random subset of features
  - Trees vote on final prediction
- **Training Time**: Depends on dataset size (typically seconds to minutes)

##### **Step 7: Model Evaluation**
- **Predictions**: `y_pred = model.predict(X_test)`
- **Metrics Calculated**:

  1. **Accuracy**: Overall correctness
     ```
     Accuracy = (TP + TN) / (TP + TN + FP + FN)
     ```
   
  2. **Confusion Matrix**: 
     ```
                 Predicted
               0 (Serverless)  1 (Traditional)
     Actual 0      TN              FP
           1      FN              TP
     ```
     - **TP (True Positives)**: Correctly predicted traditional
     - **TN (True Negatives)**: Correctly predicted serverless
     - **FP (False Positives)**: Incorrectly predicted traditional (was serverless)
     - **FN (False Negatives)**: Incorrectly predicted serverless (was traditional)

  3. **Classification Report** (per-class metrics):
     - **Precision**: Of all predicted positives, how many were correct?
       ```
       Precision = TP / (TP + FP)
       ```
     - **Recall (Sensitivity)**: Of all actual positives, how many were found?
       ```
       Recall = TP / (TP + FN)
       ```
     - **F1 Score**: Harmonic mean of precision and recall
       ```
       F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
       ```
     - **Support**: Number of actual occurrences of each class

##### **Step 8: Feature Importance Analysis**
- **Process**: 
  - Extracts feature importance scores from trained model
  - Importance = average reduction in impurity across all trees
  - Higher score = more influential in predictions
- **Visualization**: 
  - Creates horizontal bar chart
  - Saves as PNG: `orchestrator_model_feature_importance.png`
  - Shows which features most influence platform selection
- **Interpretation**: 
  - Features with high importance are key decision factors
  - Helps understand model behavior
  - Can guide feature engineering improvements

##### **Step 9: Model Persistence**
- **Format**: Pickle file (`.pkl`) using `joblib.dump()`
- **Why joblib**: More efficient than standard pickle for NumPy arrays
- **File**: `orchestrator_model.pkl`
- **Usage**: Can be loaded later for predictions without retraining

#### **Training Command Examples**:

```bash
# Basic training
cd model
python train_model.py --input ../data/workload_dataset_sample.csv --output orchestrator_model.pkl

# Using wrapper script
python run_model.py
```

#### **Expected Training Output**:

```
Loading dataset: ../data/workload_dataset_sample.csv

Training RandomForest model...

Model Evaluation Results:
Accuracy: 85.50%

Confusion Matrix:
[[15  2]
 [ 3 20]]

Classification Report:
              precision    recall  f1-score   support

   Serverless       0.83      0.88      0.86        17
  Traditional       0.91      0.87      0.89        23

    accuracy                           0.88        40
   macro avg       0.87      0.88      0.87        40
weighted avg       0.88      0.88      0.88        40

Model saved to: orchestrator_model.pkl
Feature importance plot saved to: orchestrator_model_feature_importance.png

Top Contributing Features:
 - cost ratio: 0.3245
 - execution time: 0.2156
 - cpu cores: 0.1892
 - memory mb: 0.1456
 - latency sensitive: 0.0789
 - data size mb: 0.0462
```

#### **Model Performance Interpretation**:

- **High Accuracy (>80%)**: Model performs well on test data
- **Balanced Precision/Recall**: Model doesn't favor one class over another
- **Feature Importance Insights**:
  - Cost ratio typically most important (economic efficiency)
  - Execution time second (serverless timeout constraints)
  - CPU and memory follow (resource constraints)
  - Latency sensitivity and data size less critical but still useful

#### **Model Integration** (Future Enhancement):

**Current State**: Backend uses rule-based heuristics for predictions

**To Integrate ML Model**:
1. Load model in `backend/app.py`:
   ```python
   import joblib
   model = joblib.load('model/orchestrator_model.pkl')
   ```
2. Replace heuristic logic in `/predict` endpoint:
   ```python
   features = [[cpu, mem_mb, latency, time_s, data_size_mb, cost_ratio]]
   prediction = model.predict(features)[0]
   platform = "Serverless" if prediction == 0 else "Traditional"
   ```
3. Benefits: More nuanced predictions, learns from data patterns

#### **Model Retraining**:

**When to Retrain**:
- New data becomes available
- Cost constants change
- Platform constraints change
- Model performance degrades

**Retraining Process**:
1. Update preprocessing with new data
2. Run training script again
3. Compare new model performance with previous
4. Deploy if performance improved

### 6. **Web User Interface**

**Main Page** (`/`):
- Input form for workload parameters
- Real-time prediction results
- Preset buttons (Serverless/Traditional examples)
- Deploy bundle download button
- Processing indicators

**Benchmark Page** (`/benchmark`):
- Interactive chart visualization
- Tabbed interface (Chart, Confusion Matrix, Metrics)
- Metric toggles (Efficiency, Cost, Latency)
- Summary statistics panel

**UI Features**:
- Dark theme with modern design
- Responsive layout
- Accessibility support (ARIA labels)
- Loading states and error handling
- Form validation

---

## üì¶ Installation & Setup

### Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- Web browser (Chrome, Firefox, Edge, etc.)

### Step 1: Install Backend Dependencies

```bash
cd backend
pip install -r requirements.txt
```

**Dependencies**:
- Flask
- scikit-learn
- pandas

### Step 2: Install Development Dependencies (for testing)

```bash
# From project root
pip install -r requirements-dev.txt
```

**Development Dependencies**:
- pytest

### Step 3: Verify Data Files

Ensure the following files exist:
- `data/Sample_Dataset.csv` (input dataset)
- `data/workload_dataset_sample.csv` (will be generated during preprocessing)

---

## üöÄ Running the Project

### Option 1: Run the Web Application (Main Method)

**Start the Flask server**:

```bash
# From project root
cd backend
python app.py
```

**Or from project root**:

```bash
python backend/app.py
```

**Expected Output**:
```
 * Running on http://127.0.0.1:5000
 * Debug mode: on
```

**Access the Application**:
- Main page: http://127.0.0.1:5000
- Benchmark page: http://127.0.0.1:5000/benchmark

### Option 2: Preprocess Data (Optional - for model training)

**Run preprocessing**:

```bash
cd preprocessing
python run.py
```

**Or manually**:

```bash
cd preprocessing
python preprocess.py --input ../data/Sample_Dataset.csv --output ../data/workload_dataset_sample.csv --cores 16 --mem_mb 65536
```

**Custom machine configuration**:

```bash
python preprocess.py --input ../data/Sample_Dataset.csv --output ../data/workload_dataset_sample.csv --cores 32 --mem_mb 131072 --seed 42
```

### Option 3: Train the ML Model (Optional)

**Train the model**:

```bash
cd model
python run_model.py
```

**Or manually**:

```bash
cd model
python train_model.py --input ../data/workload_dataset_sample.csv --output orchestrator_model.pkl
```

**Expected Output**:
- Model saved to `model/orchestrator_model.pkl`
- Feature importance plot: `model/orchestrator_model_feature_importance.png`
- Classification metrics printed to console

---

## üß™ Testing the Project

### Run Unit Tests

**Run all tests**:

```bash
# From project root
pytest tests/
```

**Run specific test file**:

```bash
pytest tests/test_deploy.py
```

**Run with verbose output**:

```bash
pytest tests/ -v
```

**Run with coverage** (if pytest-cov is installed):

```bash
pytest tests/ --cov=backend --cov-report=html
```

### Test Coverage

The test suite (`tests/test_deploy.py`) includes:

1. **`test_render_deployment_files_serverless()`**:
   - Verifies serverless deployment files are generated correctly
   - Checks for `lambda_handler.py` and `serverless.yml`
   - Validates memory and timeout constraints

2. **`test_render_deployment_files_traditional()`**:
   - Verifies traditional deployment files are generated correctly
   - Checks for `run_job.py`
   - Validates CPU and memory limits in docker-compose.yml

3. **`test_deploy_endpoint_creates_zip()`**:
   - Tests the `/deploy` API endpoint
   - Verifies ZIP file generation
   - Checks file contents in the bundle

### Manual Testing via Web UI

1. **Test Prediction**:
   - Navigate to http://127.0.0.1:5000
   - Fill in workload parameters
   - Click "Predict" button
   - Verify results display correctly

2. **Test Presets**:
   - Click "Serverless Preset" or "Traditional Preset"
   - Verify fields are populated
   - Click "Predict" to see recommendations

3. **Test Deployment Bundle**:
   - Enter workload parameters
   - Click "Deploy Bundle"
   - Verify ZIP file downloads
   - Extract and verify contents

4. **Test Benchmark Dashboard**:
   - Navigate to http://127.0.0.1:5000/benchmark
   - Verify charts load
   - Test metric toggles (Efficiency, Cost, Latency)
   - Test tab switching (Chart, Confusion Matrix, Metrics)

### API Testing with cURL

**Test `/predict` endpoint**:

```bash
curl -X POST http://127.0.0.1:5000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "cpu_cores": 1.5,
    "memory_mb": 1024,
    "latency_sensitive": 0,
    "execution_time": 120,
    "data_size_mb": 300
  }'
```

**Test `/deploy` endpoint**:

```bash
curl -X POST http://127.0.0.1:5000/deploy \
  -H "Content-Type: application/json" \
  -d '{
    "cpu_cores": 1.5,
    "memory_mb": 1024,
    "latency_sensitive": 0,
    "execution_time": 120,
    "data_size_mb": 300
  }' \
  --output deploy_bundle.zip
```

**Test `/benchmark-data` endpoint**:

```bash
curl http://127.0.0.1:5000/benchmark-data
```

---

## üîÑ Project Workflow

### Complete Workflow (End-to-End)

#### **Phase 1: Data Preparation**

1. **Data Preprocessing**:
   ```bash
   cd preprocessing
   python run.py
   ```
   - Processes raw dataset
   - Generates `workload_dataset_sample.csv`
   - **Time**: 1-5 minutes depending on dataset size
   - **Output**: Clean, standardized CSV ready for ML

2. **Data Validation** (Manual Check):
   - Verify output CSV has expected columns
   - Check for reasonable value ranges
   - Ensure target platform distribution is balanced
   - Review preprocessing logs for warnings

#### **Phase 2: Model Development** (Optional)

3. **Model Training**:
   ```bash
   cd model
   python run_model.py
   ```
   - Trains RandomForest classifier
   - Generates `orchestrator_model.pkl`
   - Generates feature importance visualization
   - **Time**: 30 seconds - 2 minutes
   - **Output**: Trained model + evaluation metrics

4. **Model Evaluation** (Review):
   - Check accuracy, precision, recall, F1 scores
   - Review confusion matrix for class balance
   - Analyze feature importance plot
   - Determine if model meets performance thresholds

#### **Phase 3: Application Deployment**

5. **Install Dependencies**:
   ```bash
   cd backend
   pip install -r requirements.txt
   ```

6. **Start Web Application**:
   ```bash
   cd backend
   python app.py
   ```
   - Server starts on http://127.0.0.1:5000
   - Debug mode enabled by default

#### **Phase 4: Usage & Testing**

7. **Use the Application**:
   - Enter workload parameters via web UI
   - Get platform recommendations
   - Download deployment bundles
   - View benchmark metrics

8. **Run Tests**:
   ```bash
   pytest tests/
   ```
   - Verify all functionality works correctly
   - Check deployment bundle generation

### Typical Usage Flow

1. User opens web application
2. User enters workload characteristics (CPU, memory, execution time, etc.)
3. System calculates costs for both platforms
4. System applies decision rules to recommend platform
5. User reviews recommendation and cost comparison
6. User can download deployment bundle with ready-to-use configs
7. User can view benchmark dashboard for comparative analysis

---

## üîç Important Steps & Best Practices

### **Data Quality Assurance**

#### **1. Input Data Validation**
- **Check Data Format**: Ensure input CSV has expected columns
- **Verify Data Types**: Numeric fields should be numeric, not strings
- **Handle Missing Values**: Preprocessing handles this, but verify output
- **Outlier Detection**: Review extreme values that might skew results

#### **2. Preprocessing Validation**
After running preprocessing, verify:
```python
import pandas as pd
df = pd.read_csv('data/workload_dataset_sample.csv')

# Check for missing values
print(df.isnull().sum())

# Check value ranges
print(df.describe())

# Check target distribution
print(df['target platform'].value_counts())

# Verify no infinite values
print(df.isin([np.inf, -np.inf]).sum())
```

#### **3. Feature Engineering Validation**
- **CPU Cores**: Should be positive, reasonable range (0.1 - 64)
- **Memory**: Should be positive, reasonable range (128 - 65536 MB)
- **Execution Time**: Should be positive, reasonable range (1 - 3600 seconds)
- **Cost Ratio**: Should be positive, typically 0.1 - 100
- **Latency Sensitive**: Should be binary (0 or 1)

### **Model Training Best Practices**

#### **1. Data Splitting Strategy**
- **Train/Test Split**: 80/20 ratio with stratification
- **Why Stratification**: Maintains class distribution in both sets
- **Random Seed**: Fixed (42) for reproducibility
- **Future Enhancement**: Consider k-fold cross-validation for small datasets

#### **2. Hyperparameter Tuning** (Future Enhancement)
Current model uses default hyperparameters. For improvement:
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

#### **3. Model Evaluation Metrics**
- **Primary Metric**: Accuracy (overall correctness)
- **Secondary Metrics**: Precision, Recall, F1 (per-class performance)
- **Confusion Matrix**: Visual representation of errors
- **Feature Importance**: Understanding model decisions

#### **4. Model Validation**
- **Test Set Performance**: Should be similar to training performance
- **Overfitting Check**: Large gap between train/test accuracy indicates overfitting
- **Class Balance**: Ensure both classes are well-represented
- **Error Analysis**: Review misclassified examples to understand failure modes

### **Feature Engineering Details**

#### **1. Cost Ratio Feature**
- **Purpose**: Captures economic efficiency
- **Calculation**: `cost_serverless / cost_traditional`
- **Interpretation**: 
  - < 1.0: Serverless cheaper
  - 1.0 - 5.0: Serverless acceptable
  - > 5.0: Traditional preferred
- **Why Important**: Directly influences platform selection

#### **2. Latency Sensitivity Feature**
- **Derived From**: Priority, scheduling class, workload names
- **Binary Encoding**: 0 (not sensitive) or 1 (sensitive)
- **Impact**: Latency-sensitive workloads prefer traditional infrastructure
- **Heuristic-Based**: Could be improved with domain knowledge

#### **3. Execution Time Feature**
- **Normalized**: Always in seconds
- **Auto-Detection**: Handles various timestamp formats
- **Constraint**: Serverless platforms have timeout limits (typically 15 minutes)
- **Critical Threshold**: 300 seconds (5 minutes) for serverless suitability

#### **4. Resource Features (CPU, Memory)**
- **Normalized**: Handles both fractional and absolute values
- **Constraints**: 
  - Serverless: CPU ‚â§ 2, Memory ‚â§ 2048 MB
  - Traditional: No hard limits
- **Scaling**: Automatically scales based on machine configuration

### **Deployment Considerations**

#### **1. Model Versioning**
- **Current**: Single model file (`orchestrator_model.pkl`)
- **Best Practice**: Version models (e.g., `model_v1.pkl`, `model_v2.pkl`)
- **Metadata**: Track training date, dataset version, performance metrics

#### **2. Model Monitoring** (Future Enhancement)
- **Prediction Logging**: Track predictions and outcomes
- **Performance Drift**: Monitor if model accuracy degrades over time
- **A/B Testing**: Compare rule-based vs ML-based predictions
- **Feedback Loop**: Collect user feedback on recommendations

#### **3. Cost Constants Management**
- **Current**: Hardcoded in `backend/app.py`
- **Best Practice**: Store in configuration file or environment variables
- **Updates**: Easy to update when cloud pricing changes
- **Multi-Cloud**: Support different pricing for AWS, Azure, GCP

### **Testing & Validation Steps**

#### **1. Unit Testing**
- **Coverage**: Test individual functions (cost calculation, parsing, etc.)
- **Edge Cases**: Test with extreme values, missing data, invalid inputs
- **Current Tests**: Deployment bundle generation tests

#### **2. Integration Testing**
- **API Endpoints**: Test `/predict`, `/deploy`, `/benchmark-data`
- **Data Flow**: Test complete workflow from input to output
- **Error Handling**: Test with invalid inputs, missing fields

#### **3. Performance Testing**
- **Response Time**: API should respond in < 1 second
- **Concurrent Requests**: Test with multiple simultaneous users
- **Model Inference**: ML predictions should be fast (< 100ms)

#### **4. User Acceptance Testing**
- **UI/UX**: Test web interface usability
- **Workflow**: Test complete user journey
- **Edge Cases**: Test with various workload configurations
- **Feedback**: Collect user feedback for improvements

### **Troubleshooting Common Issues**

#### **1. Preprocessing Issues**
- **Problem**: Missing columns in input data
  - **Solution**: Check input CSV structure, update preprocessing script
- **Problem**: All values are NaN after preprocessing
  - **Solution**: Check data format, verify parsing logic
- **Problem**: Execution time is negative or zero
  - **Solution**: Check timestamp columns, verify time unit detection

#### **2. Model Training Issues**
- **Problem**: Low accuracy (< 70%)
  - **Solution**: Check data quality, try different hyperparameters, get more data
- **Problem**: Imbalanced classes
  - **Solution**: Use class_weight="balanced", collect more data for minority class
- **Problem**: Overfitting (high train accuracy, low test accuracy)
  - **Solution**: Reduce model complexity, increase training data, use regularization

#### **3. Application Issues**
- **Problem**: Server won't start
  - **Solution**: Check port availability, verify dependencies installed
- **Problem**: Predictions are always the same
  - **Solution**: Check input validation, verify cost calculation logic
- **Problem**: Deployment bundle is empty
  - **Solution**: Check file generation logic, verify ZIP creation

### **Performance Optimization**

#### **1. Data Processing**
- **Sampling**: Current 100-row sample is sufficient for development
- **Parallel Processing**: Could parallelize preprocessing for large datasets
- **Caching**: Cache preprocessed data to avoid reprocessing

#### **2. Model Inference**
- **Current**: Rule-based (very fast, < 1ms)
- **ML Model**: RandomForest is fast (< 10ms for single prediction)
- **Optimization**: Could use lighter models (e.g., Logistic Regression) for speed

#### **3. API Performance**
- **Current**: Single-threaded Flask (development mode)
- **Production**: Use production WSGI server (Gunicorn, uWSGI)
- **Caching**: Cache model predictions for identical inputs
- **Async**: Use async frameworks for better concurrency

### **Security Considerations**

#### **1. Input Validation**
- **Current**: Basic validation in `parse_and_validate_payload()`
- **Enhancement**: Add stricter validation, sanitize inputs
- **SQL Injection**: Not applicable (no database)
- **XSS**: Frontend should sanitize user inputs

#### **2. Model Security**
- **Model Poisoning**: Validate training data sources
- **Adversarial Attacks**: Test with malicious inputs
- **Model Theft**: Protect model file, consider model encryption

#### **3. API Security**
- **Rate Limiting**: Prevent abuse (not currently implemented)
- **Authentication**: Add API keys for production (not currently implemented)
- **HTTPS**: Use HTTPS in production (not currently implemented)

---

## üìä Key Features Summary

‚úÖ **Intelligent Platform Selection**: Rule-based + ML-ready architecture  
‚úÖ **Cost Optimization**: Real-time cost calculation and comparison  
‚úÖ **Deployment Automation**: One-click deployment bundle generation  
‚úÖ **Interactive Dashboard**: Visual benchmarking and metrics  
‚úÖ **Data Processing Pipeline**: Automated preprocessing and model training  
‚úÖ **Modern Web UI**: Responsive, accessible, dark-themed interface  
‚úÖ **Comprehensive Testing**: Unit tests for critical components  

---

## üîß Configuration

### Cost Constants (in `backend/app.py`)

```python
P_CPU = 0.0316        # $ per CPU core-hour (Traditional)
P_MEM = 0.0045        # $ per GB-hour (Traditional)
P_GB_SEC = 0.00001667 # $ per GB-second (Serverless)
P_REQ = 0.0000002     # $ per request (Serverless)
```

### Serverless Constraints

```python
SERVERLESS_MEMORY_MIN = 128      # MB
SERVERLESS_MEMORY_MAX = 10240    # MB (10 GB)
SERVERLESS_TIMEOUT_MAX = 900     # seconds (15 minutes)
```

### Decision Thresholds

```python
short_runtime_threshold_s = 300  # seconds
small_cpu_threshold = 2.0        # cores
small_mem_threshold_mb = 2048    # MB
cost_ratio_threshold = 5.0       # ratio
```

---

## üìù Notes

- The current implementation uses **rule-based heuristics** for platform recommendation. The trained ML model exists but is not integrated into the prediction endpoint.
- To integrate the ML model, modify `backend/app.py` to load and use `orchestrator_model.pkl` in the `/predict` endpoint.
- The benchmark data is currently **mock data**. For production, integrate real performance metrics from actual deployments.
- All file paths use relative paths, so ensure you run commands from the correct directories.

---

## üéì Learning Outcomes

This project demonstrates:
- Hybrid cloud architecture decision-making
- Cost optimization strategies
- Machine learning model training and evaluation
- Web application development (Flask + frontend)
- Data preprocessing and feature engineering
- API design and testing
- Deployment automation

---

**Project Status**: Functional and ready for use. The system provides accurate cost calculations and platform recommendations based on workload characteristics.


